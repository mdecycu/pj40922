\chapter{有限元素法}
用於求解偏微分方程或積分方程組數值求解，將大型物理系統細分為更小、更簡單的部分，稱為有限元，通過在空間維度上進行特定的空間離散化，有限元素法通過最小化關聯的誤差函數，使用來自變異演算的變異方法來近似求，來實現用於複雜的工程結構或物理系統的行為和性能。\\

%---------------------基本概念及假設假定-------------------------%
\section{基本概念及假設假定}
1.離散化:將物理系統或結構分為數格小元素，每個元素都能進行數學建模及計算\\
2.有限元素:通常由三角形或者四邊形等形成結構的區域\\
3.節點:每個元素的角落或中心點，用於連接元素之間的邊界條件和解決方程\\
4.變形:系統在外力的影響下發生的形變，經過分析後計算每個元素的變形及變形之間的相互引響預測整得系統的變形及應力、各種參數的分布\\
5.自由度: 節點上變量的個數，例如用位移法解結構問題時節點自由度為3，表示單個節點上三個坐標方向上的位移，又例如熱分析時節點自由度為1，表示某個節點處的溫度值\\
6.邊界條件:物理系統邊界上的約束和力，模擬實際問題中的條件及約束\\
7.材料特性:物理系統(模型)的材料性質，不同的材料其中的參數各為不同(彈性模數、潽淞比、極限強度、屈服強度等)。\\

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=16cm]{類神經網路架構}
\caption{\Large 類神經網路架構}\label{類神經網路架構}
\end{center}
\end{figure}
\newpage

\begin{figure}
\begin{center}
\includegraphics[width=16cm]{類神經網路架構}
\caption{\Large 類神經網路架構}
\label{類神經網路架構}
\end{center}
\end{figure}
\begin{itemize}
%=----------Sigmoid      Function----------=%
\item Sigmoid Function(圖.\ref{SigmoidFunction})：\\
$$\sigma(x)=\frac{1}{1+e^{-x}}$$

%=----------SigmoidPrime Function----------=%
$$\sigma^{'}(x)=\sigma(x)[1-\sigma(x)]$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=16cm]{SigmoidFunction}
\caption{\Large SigmoidFunction}\label{SigmoidFunction}
\end{center}
\end{figure}
\\
%=----------Softmax Function----------=%
\item Softmax：\\
$$S(x)=\frac{e^{x_i}}{\sum^k_{j=1}e^{x_i}}$$
%=----------Relu Function----------=%
\item ReLU Function：\\
\end{itemize}
$$f(x)=max(0,x)$$
$$if , x<0 , f(x)=0$$
$$else f(x)=x$$

%=----------Faces of Reinforcement Learning---------------=%

強化學習涵蓋範圍(圖.\ref{各領域與機器學習應用範圍})：\\
 
%======需文字補充========%
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=11cm]{Faces_of_Reinforcement_Learning}
\caption{\Large 各領域與機器學習應用範圍}
\label{各領域與機器學習應用範圍}
\end{center}
\end{figure}
%=--------The Flow of Reinforcement Learning------------=%.
\newpage
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=12cm]{The_Flow_of_Reinforcement_Learning}
\caption{\Large 強化學習架構}
\label{RL structur}
\end{center}
\end{figure}
\newpage
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{The_entire_interaction_process}
\caption{\Large 整個互動過程}
\label{整個互動過程}
\end{center}
\end{figure}

 
%------------------圖片可共用----------------------%
\iffalse
\begin{figure}[hbt!]
\begin{center}
\includegraphics[scale=0.74]{ Reinforcement_Learning_interactions}
\caption{\Large Reinforcement Learning interactions}
\end{center}
\end{figure}
\fi
%=----Interactions with Reinforcement Learning------------=%


\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{agent}
\caption{\Large agent}
\label{agent}
\end{center}
\end{figure}
%=----Agents------------=%
\begin{flushleft}
\end{flushleft}

%--------------------------方程介紹--------------------------%
\section{方程介紹}
%=========表格=========%
\begin{center}
\begin{tabular}[c]{ccc}    
%\multicolumn{1}{r}{MRP}
 MRP & $\longleftrightarrow$ & MDP\\
\hline
$P^{\pi}(s's)$ & = & $\sum_{a\in A}\pi (a|s)P(s'|s, a)$\\
$P^{\pi}(s)$ & = & $\sum_{a\in A}\pi (a|s)P(s, a)$\\
\includegraphics[height=3cm]{MRP}&&\includegraphics[height=3cm]{MDP}\\
\hline
\end{tabular}
\end{center}
\hspace{15pt}
 
state value function(狀態值方程式)$v^{\pi}(s)$\\
$$v^{\pi}(s) = \mathbb{E}[G_t|s_t=s]$$
$$= \mathbb{E}[R_{t+1}+\gamma v^{\pi}(s_{t+1})|s_t=s]$$
$$= \sum_{a\in A}\pi (a|s)q^{\pi}(s, a)$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=8cm]{s_to _s}
\caption{$v^{\pi}$程序圖}
\label{fig.s_to_s}
\end{center}
\end{figure}
$$v^{\pi}(s) = \sum_{a\in A}\pi (a|s)(R(s, a)+\gamma \sum_{s'\in s}P(s'|s, a)v^{\pi}(s'))$$
\newpage
state value function(狀態值方程式)$q^{\pi}(s)$
$$v^{\pi}(s) = \mathbb{E}[G_t|s_t=s]$$
$$= \mathbb{E}[R_{t+1}+\gamma v^{\pi}(s_{t+1})|s_t=s]$$
$$= \sum_{a\in A}\pi (a|s)q^{\pi}(s, a)$$
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=8cm]{Q_pi function}
\caption{$q^{\pi}$程序圖}
\label{fig.q_pi}
\end{center}
\end{figure}
$$q^\pi(s, a)=R(s, a)+\gamma\sum_{s'\in S}P(s'|s, a)\sum_{a'\in A}\pi(a'|s')q^{\pi}(s', a')$$
\newpage
%----------------------分析步驟--------------------------%
\section{分析步驟}

1.建立幾何模型:將需求解的問題繪製成幾何模型，以透過二維或三維帶入\
2.離散化:將模型分成數個元素\
3.導入有限元方程:對每個需求解的模型或問題不相同，根據力學性質或物理方程帶入相應的關西式(應力、應變、變形、熱傳等)\
4.裝配方程組:將所有單元方程組裝成一個大的線性方程，對問題進行代數求解\
5.求解方程組:透過線性方程，得到節點處的位移及負載等相關訊息\
6.後處理:對求解結果進行誤差分析及驗證\
\newpage

\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{policy gradient原理}
\caption{\Large Policy Gradient原理}
\label{Policy Gradient原理}
\end{center}
\end{figure}
\newpage
%-----------------------有限元素分析原理--------------------------%
\section{有限元素分析原理}
\begin{figure}[hbt!]
\begin{center}
\includegraphics[width=15cm]{policy gradient原理}
\caption{\Large Policy Gradient原理}
\label{Policy Gradient原理}
\end{center}
\end{figure}
